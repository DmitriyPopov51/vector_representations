{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e01f2f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# выбор видеокарты\n",
    "# оставляет видимой только одну GPU \n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import torch\n",
    "# проверка на количество видеокарт (если 1 то все ок, если 2 то надо смотреть)\n",
    "print('CUDA_VISIBLE_DEVICES', torch.cuda.device_count())\n",
    "print('CURRENT DEVICE', torch.cuda.current_device())\n",
    "print('____________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c886b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753cac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from longformer.longformer import Longformer, LongformerConfig\n",
    "from longformer.sliding_chunks import pad_to_window_size\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81046ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LongformerConfig.from_pretrained('../models/longformer-base-4096/') \n",
    "config.attention_mode = 'sliding_chunks'\n",
    "\n",
    "model = Longformer.from_pretrained('../models/longformer-base-4096/', config=config)\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# tokenizer.model_max_length = model.config.max_position_embeddings\n",
    "tokenizer.model_max_length = 200000\n",
    "\n",
    "model = model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5f09e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# токенизируем текст\n",
    "# text - текст в формате строки\n",
    "# add_special_tokens - True or False, добавлять специальные токены или нет (<cls> <s>)\n",
    "# НА ВЫХОД:\n",
    "# tokens - список из токенов\n",
    "# pad_token_id - токен необходимый в модели\n",
    "def tokenize_text(text, add_special_tokens, tokenizer):\n",
    "    if add_special_tokens == True:\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "    else:\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    return tokens, tokenizer.pad_token_id\n",
    "\n",
    "# сокращение списка токенов до списка <= 4096 \n",
    "# tokens_list - список токенов \n",
    "# new_size - размерность списка, до которого надо сократить\n",
    "# НА ВЫХОД:\n",
    "# new_tokens - обрезанный список с токенами (<= new_size)\n",
    "#     выглядит в формате вложенного списка [[0,1,2]]\n",
    "def cut_tokens(tokens_list, new_size):\n",
    "    new_tokens = []\n",
    "    tok = tokens_list[:new_size]\n",
    "    new_tokens.append(tok)\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3062d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# открыть файл\n",
    "# id - id файла\n",
    "# folder - путь к папке ('text')\n",
    "# на ВЫХОД:\n",
    "# text_in_string текст в формате строки\n",
    "def open_file(d_id, folder):\n",
    "    path = folder + '/' + str(d_id) + '.txt'\n",
    "    text_file = open(path,'r')\n",
    "    text_in_string = text_file.read()\n",
    "    text_file.close()\n",
    "    return text_in_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff8e616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# формируем список пар текстов для обучения модели\n",
    "# path_dev_file - путь до файла dev.csv\n",
    "# НА ВЫХОД\n",
    "# train_dl - список текстов\n",
    "# (4090 токенов текста1, 4090 токенов текста2, label)\n",
    "def create_data_for_training(path_dev_file, texts_folder):\n",
    "    model_size = 2000\n",
    "    dev_df = pd.read_csv(path_dev_file)\n",
    "    train_dl = []\n",
    "    len_dev_df = len(dev_df)\n",
    "    print(\"Text preprocessing\")\n",
    "    for i in dev_df.index:\n",
    "#     i = 0\n",
    "#     while i < 5:\n",
    "        doc_id_1 = dev_df.take([i]).values[0][0] #doc_id_1\n",
    "        doc_id_2 = dev_df.take([i]).values[0][2] #doc_id_2\n",
    "        label = dev_df.take([i]).values[0][4] #label\n",
    "        \n",
    "#         text1 = open_file(doc_id_1, texts_folder)\n",
    "#         text2 = open_file(doc_id_2, texts_folder)\n",
    "#         elem = (text1, text2, label)\n",
    "        \n",
    "        text1 = open_file(doc_id_1, texts_folder)\n",
    "        tokens1, pad_token_id1 = tokenize_text(text1, True, tokenizer)\n",
    "        cut_tok1 = cut_tokens(tokens1, model_size)\n",
    "#         input_ids1 = torch.tensor(cut_tok1).unsqueeze(0)\n",
    "#         text_cut1 = tokenizer.decode(input_ids1[0][0])\n",
    "        \n",
    "        text2 = open_file(doc_id_2, texts_folder)\n",
    "        tokens2, pad_token_id2 = tokenize_text(text2, True, tokenizer)\n",
    "        cut_tok2 = cut_tokens(tokens2, model_size)\n",
    "#         input_ids2 = torch.tensor(cut_tok2).unsqueeze(0)\n",
    "#         text_cut2 = tokenizer.decode(input_ids2[0][0])\n",
    "        \n",
    "#         elem = (text_cut1, text_cut2, label)\n",
    "        elem = (cut_tok1[0], cut_tok2[0], label)\n",
    "#         elem = (input_ids1[0], input_ids2[0], label)\n",
    "        train_dl.append(elem)\n",
    "#         i += 1\n",
    "        \n",
    "#         time.sleep(0.0000001)\n",
    "        print('\\r', 'iter... {} / {}'.format(str(i), len_dev_df - 1, end=''))\n",
    "    \n",
    "    return train_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55253c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount_label - сколько экземпляров 0 и сколько экземпляров 1 надо добавить в тренировочную базу\n",
    "def create_data_for_training1(path_dev_file, texts_folder, amount_label):\n",
    "    model_size = 2000\n",
    "    df = pd.read_csv(path_dev_file)\n",
    "    df0 = df[df['label'] == 0]\n",
    "    df1 = df[df['label'] == 1]\n",
    "    train_dl = []\n",
    "    print(\"Text preprocessing\")\n",
    "    counter = 0\n",
    "    for i in range(0, len(df0), len(df0)//amount_label):\n",
    "        if len(train_dl) >= amount_label:\n",
    "            break\n",
    "        doc_id_1 = df0.take([i]).values[0][0] #doc_id_1\n",
    "        doc_id_2 = df0.take([i]).values[0][2] #doc_id_2\n",
    "        label = df0.take([i]).values[0][4] #label\n",
    "        \n",
    "        text1 = open_file(doc_id_1, texts_folder)\n",
    "        tokens1, pad_token_id1 = tokenize_text(text1, True, tokenizer)\n",
    "        cut_tok1 = cut_tokens(tokens1, model_size)\n",
    "        text2 = open_file(doc_id_2, texts_folder)\n",
    "        tokens2, pad_token_id2 = tokenize_text(text2, True, tokenizer)\n",
    "        cut_tok2 = cut_tokens(tokens2, model_size)\n",
    "        \n",
    "        elem = (cut_tok1[0], cut_tok2[0], label)\n",
    "\n",
    "        train_dl.append(elem)\n",
    "        \n",
    "        counter += 1\n",
    "        print('\\r', 'iter... {} / {}'.format(str(counter), amount_label*2), end='          ')\n",
    "        \n",
    "    \n",
    "    for i in range(0, len(df1), len(df1)//amount_label):\n",
    "        if len(train_dl) >= amount_label*2:\n",
    "            break\n",
    "        doc_id_1 = df1.take([i]).values[0][0] #doc_id_1\n",
    "        doc_id_2 = df1.take([i]).values[0][2] #doc_id_2\n",
    "        label = df1.take([i]).values[0][4] #label\n",
    "        \n",
    "        text1 = open_file(doc_id_1, texts_folder)\n",
    "        tokens1, pad_token_id1 = tokenize_text(text1, True, tokenizer)\n",
    "        cut_tok1 = cut_tokens(tokens1, model_size)\n",
    "        text2 = open_file(doc_id_2, texts_folder)\n",
    "        tokens2, pad_token_id2 = tokenize_text(text2, True, tokenizer)\n",
    "        cut_tok2 = cut_tokens(tokens2, model_size)\n",
    "        \n",
    "        elem = (cut_tok1[0], cut_tok2[0], label)\n",
    "\n",
    "        train_dl.append(elem)\n",
    "\n",
    "        counter += 1\n",
    "        print('\\r', 'iter... {} / {}'.format(str(counter), amount_label*2), end='          ')\n",
    "    \n",
    "    return train_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78546c6e",
   "metadata": {},
   "source": [
    "### Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b737286",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataset_v1\n",
    "# path_dev_file = '../data/dataset_v1/dev.csv'\n",
    "# path_train_file = '../data/dataset_v1/train.csv'\n",
    "# texts_folder = '../data/dataset_v1/text'\n",
    "\n",
    "# SimRuWiki\n",
    "# path_dev_file = '../data/SimRuWiki/dev.csv'\n",
    "# path_train_file = '../data/SimRuWiki/train.csv'\n",
    "# texts_folder = '../data/SimRuWiki/texts'\n",
    "\n",
    "# SimEnWiki\n",
    "path_dev_file = '../data/SimEnWiki/dev.csv'\n",
    "path_train_file = '../data/SimEnWiki/train.csv'\n",
    "texts_folder = '../data/SimEnWiki/texts'\n",
    "\n",
    "# dev_df = pd.read_csv(path_dev_file)\n",
    "# train_dl = create_data_for_training(path_train_file, texts_folder)\n",
    "# train_dl = create_data_for_training(path_dev_file, texts_folder)\n",
    "\n",
    "amount_label = 1500\n",
    "train_dl = create_data_for_training1(path_train_file, texts_folder, amount_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aa8de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dl[:1][0][0]\n",
    "len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dda9ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "count0 = 0\n",
    "count1 = 0\n",
    "for i, (text1, text2, true_label) in enumerate(train_dl):\n",
    "    if true_label==0:\n",
    "        count0 += 1\n",
    "    elif true_label==1:\n",
    "        count1 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6000de",
   "metadata": {},
   "outputs": [],
   "source": [
    "count0, count1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b958d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCELoss, BCEWithLogitsLoss, Sigmoid, CosineEmbeddingLoss\n",
    "from torch.optim import SGD\n",
    "from scipy.spatial.distance import cosine \n",
    "import time\n",
    "import numpy as np\n",
    "# train the model\n",
    "def train_model(train_dl, model):\n",
    "    \n",
    "#     m = Sigmoid()\n",
    "    \n",
    "    # define the optimization\n",
    "    \n",
    "    model = model.cuda()\n",
    "    criterion = BCELoss()\n",
    "#     criterion = CosineEmbeddingLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=3e-5, momentum=0.9)\n",
    "#     model = model.cuda()\n",
    "    model.train()\n",
    "    cos = torch.nn.CosineSimilarity(dim=0)\n",
    "#     model.zero_grad()\n",
    "    # enumerate epochs\n",
    "    num_epoch = 1\n",
    "    for epoch in range(num_epoch):\n",
    "        print('\\nepoch {}/{}'.format(epoch+1, num_epoch))\n",
    "#         print(torch.cuda.memory_summary())\n",
    "        for i, (text1, text2, true_label) in enumerate(train_dl):\n",
    "            \n",
    "#             temp_cls = torch.tensor([1.])\n",
    "#             temp_cls = np.array([0.])\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            for j in range(2):\n",
    "                if j % 2 == 0:\n",
    "                    # это первый прогон, тут оставляем без градиента\n",
    "                    input_ids1 = torch.tensor(text1).unsqueeze(0)\n",
    "                    input_ids1 = input_ids1.cuda()\n",
    "                    attention_mask1 = torch.ones(input_ids1.shape, dtype=torch.long, device=input_ids1.device) # initialize to local attention\n",
    "                    attention_mask1[:, [0]] =  2\n",
    "                    input_ids1, attention_mask1 = pad_to_window_size(input_ids1, attention_mask1, config.attention_window[0], tokenizer.pad_token_id)\n",
    "                    \n",
    "                    output1 = model(input_ids1, attention_mask=attention_mask1)[0]\n",
    "                    cls1 = output1[0][0]\n",
    "#                     cls1 = torch.reshape(cls1, (1, len(cls1)))\n",
    "                    temp_cls = cls1.cpu().detach().numpy()\n",
    "                    # сохраняем этот токен вне цикла\n",
    "#                     cls11 = torch.tensor(cls1, requires_grad=False)\n",
    "#                     temp_cls = cls1\n",
    "                    del output1\n",
    "                    del cls1\n",
    "#                     del cls11\n",
    "                    del input_ids1\n",
    "                    del attention_mask1\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "                else:\n",
    "                    # это второй прогон, тут градиент оставляем и считаем лосс\n",
    "                    input_ids2 = torch.tensor(text2).unsqueeze(0)\n",
    "                    input_ids2 = input_ids2.cuda()\n",
    "                    attention_mask2 = torch.ones(input_ids2.shape, dtype=torch.long, device=input_ids2.device)\n",
    "                    attention_mask2[:, [0]] =  2\n",
    "                    input_ids2, attention_mask2 = pad_to_window_size(input_ids2, attention_mask2, config.attention_window[0], tokenizer.pad_token_id)\n",
    "\n",
    "                    output2 = model(input_ids2, attention_mask=attention_mask2)[0]\n",
    "                    cls2 = output2[0][0]\n",
    "#                     cls2 = torch.reshape(cls2, (1, len(cls2)))\n",
    "                    del output2\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                    # считаем лосс (сначала косинусное сходство)\n",
    "#                     temp_cls = temp_cls.to(device=cls2.device)\n",
    "                    temp_cls = torch.tensor(temp_cls, device=cls2.device)                    \n",
    "            \n",
    "#                     делим на два чтобы распределение сделать от 0 до 1\n",
    "#                     dist = float(1 - cosine(cls1, cls2)) / 2\n",
    "                    dist = cos(temp_cls, cls2)\n",
    "#                     print('cls2 size=', cls2.size())\n",
    "#                     print('dist=', dist)\n",
    "                    # сравнение с порогом для жесткого штрафа модели (делаем либо 0 либо 1)\n",
    "                    threshold = torch.tensor(0.7)\n",
    "# #                     print('thre=', threshold)\n",
    "                    if dist > threshold: # dist = 1\n",
    "#                         difference = torch.tensor(1-float(dist))\n",
    "#                         dist = dist + difference\n",
    "                        dist = torch.tensor(1., requires_grad = True)\n",
    "                    else: # dist = 0\n",
    "# #                         dist = dist - dist\n",
    "                        dist = torch.tensor(0., requires_grad = True)\n",
    "\n",
    "\n",
    "#                     loss_input = dist\n",
    "                    loss_target = torch.tensor(true_label).float()\n",
    "#                     print('dist=', dist)\n",
    "#                     print('targe=', loss_target)\n",
    "                    # without Sigmoid\n",
    "                    loss = criterion(dist, loss_target) # 0 or 1\n",
    "                    #with Sigmoid\n",
    "#                     loss = criterion(m(loss_input), loss_target) # 0 or 1\n",
    "#                     print('input=', loss_input)\n",
    "#                     print('targe=', loss_target)\n",
    "#                     print('loss=', loss)\n",
    "                    del true_label\n",
    "                    del temp_cls\n",
    "                    del input_ids2\n",
    "                    del attention_mask2\n",
    "                    del cls2\n",
    "                    del dist\n",
    "#                     del loss_input\n",
    "                    del loss_target\n",
    "                    torch.cuda.empty_cache()\n",
    "                    # credit assignment\n",
    "                    loss.backward()\n",
    "            \n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # вывод процесса итераций в консоль\n",
    "            print('\\r', 'iter... {} / {}, loss = {}'.format(str(i), len(train_dl)-1, loss.item()), end='                   ')\n",
    "            del loss\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65147fcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(train_dl, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf80c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение модели в путь save_model_to\n",
    "\n",
    "# save_model_to = '../models/longformer-base-4096-epoch1-lr3e-5-cosemblos-dataset1/'\n",
    "# save_model_to = '../models/longformer-base-4096-epoch4-lr3e-5-cosemblos-dataset-train-3500/'\n",
    "save_model_to = '../models/longformer-base-4096-epoch1-lr3e-5-bceloss-simenwiki-train-on-3000-notdel2/'\n",
    "# # save_model_to = '../models/longformer-base-4096-epoch1-lr3e-5-cosemblos-dataset2/'\n",
    "model.save_pretrained(save_model_to)\n",
    "tokenizer.save_pretrained(save_model_to)\n",
    "\n",
    "# КОДИРОВКА НАИМЕНОВАНИЯ ТРЕНИРОВАННОЙ МОДЕЛИ\n",
    "# epoch1 - количество эпох 1+\n",
    "# lr3e-5 - learning rate 3е-5 \n",
    "# sigm or nosigm - выбираем с сигмоидой или без\n",
    "# pen08 - использование жесткого штрафа модели с порогом 0.8\n",
    "\n",
    "# save_model_to = '../models/longformer-base-4096-epoch1-lr3e-4-sigm-pen08/'\n",
    "# save_model_to = '../models/longformer-base-4096-epoch1-lr3e-4-sigm/'\n",
    "# save_model_to = '../models/longformer-base-4096-epoch1-lr3e-5-nosigm/'\n",
    "# model.save_pretrained(save_model_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622909eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
